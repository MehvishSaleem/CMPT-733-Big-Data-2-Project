{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib3\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Paste Public Page Name:NHL\n",
      "Please Paste Your Access Token:EAACEdEose0cBAE5ta38GqCYv7xEuRdhbsZCAMd0XkZCu7wh5to7jPALuGmTpoR9HHBInSt0jzgkVp7Yqq8LDlLcB7QZBIq9YEQixdlz6ZCZBdF1MV1YJE1LNNq6kUNtouVaXcp9sX6ZArqim8UVGO4VM5ovZB3GPmrAEdJLNxD0ifqPaZBM4FsbnQYhIesUku5hr7SRjM5xO4AZDZD\n",
      "Scraping NHL Facebook Page: 2018-04-01 22:44:03.079758\n",
      "\n",
      "100 Statuses Processed: 2018-04-01 22:44:27.349966\n",
      "200 Statuses Processed: 2018-04-01 22:44:47.434648\n",
      "300 Statuses Processed: 2018-04-01 22:45:03.574639\n",
      "400 Statuses Processed: 2018-04-01 22:45:21.006041\n",
      "500 Statuses Processed: 2018-04-01 22:45:38.606807\n",
      "600 Statuses Processed: 2018-04-01 22:46:01.186142\n",
      "700 Statuses Processed: 2018-04-01 22:46:17.736431\n",
      "800 Statuses Processed: 2018-04-01 22:46:33.810526\n",
      "900 Statuses Processed: 2018-04-01 22:46:50.813641\n",
      "1000 Statuses Processed: 2018-04-01 22:47:11.940592\n",
      "1100 Statuses Processed: 2018-04-01 22:47:31.514895\n",
      "1200 Statuses Processed: 2018-04-01 22:47:48.311435\n",
      "1300 Statuses Processed: 2018-04-01 22:48:04.815663\n",
      "1400 Statuses Processed: 2018-04-01 22:48:22.489860\n",
      "1500 Statuses Processed: 2018-04-01 22:48:42.691818\n",
      "1600 Statuses Processed: 2018-04-01 22:48:43.505117\n",
      "1700 Statuses Processed: 2018-04-01 22:48:44.558729\n",
      "1800 Statuses Processed: 2018-04-01 22:48:45.430292\n",
      "1900 Statuses Processed: 2018-04-01 22:48:46.310110\n",
      "2000 Statuses Processed: 2018-04-01 22:48:47.196349\n",
      "2100 Statuses Processed: 2018-04-01 22:48:48.106409\n",
      "2200 Statuses Processed: 2018-04-01 22:48:49.017756\n",
      "2300 Statuses Processed: 2018-04-01 22:48:50.142241\n",
      "2400 Statuses Processed: 2018-04-01 22:48:51.093337\n",
      "2500 Statuses Processed: 2018-04-01 22:48:51.982553\n",
      "2600 Statuses Processed: 2018-04-01 22:48:52.880390\n",
      "2700 Statuses Processed: 2018-04-01 22:48:53.793107\n",
      "2800 Statuses Processed: 2018-04-01 22:48:54.881919\n",
      "2900 Statuses Processed: 2018-04-01 22:48:55.785893\n",
      "3000 Statuses Processed: 2018-04-01 22:48:56.647683\n",
      "3100 Statuses Processed: 2018-04-01 22:48:57.492954\n",
      "3200 Statuses Processed: 2018-04-01 22:48:58.411839\n",
      "3300 Statuses Processed: 2018-04-01 22:48:59.255203\n",
      "3400 Statuses Processed: 2018-04-01 22:49:00.244718\n",
      "3500 Statuses Processed: 2018-04-01 22:49:01.109073\n",
      "3600 Statuses Processed: 2018-04-01 22:49:02.152182\n",
      "3700 Statuses Processed: 2018-04-01 22:49:03.130403\n",
      "3800 Statuses Processed: 2018-04-01 22:49:04.050948\n",
      "3900 Statuses Processed: 2018-04-01 22:49:05.357281\n",
      "4000 Statuses Processed: 2018-04-01 22:49:06.360988\n",
      "4100 Statuses Processed: 2018-04-01 22:49:07.172702\n",
      "4200 Statuses Processed: 2018-04-01 22:49:08.041811\n",
      "4300 Statuses Processed: 2018-04-01 22:49:08.941753\n",
      "4400 Statuses Processed: 2018-04-01 22:49:09.753651\n",
      "4500 Statuses Processed: 2018-04-01 22:49:10.663867\n",
      "4600 Statuses Processed: 2018-04-01 22:49:11.534447\n",
      "4700 Statuses Processed: 2018-04-01 22:49:12.552860\n",
      "4800 Statuses Processed: 2018-04-01 22:49:13.383911\n",
      "4900 Statuses Processed: 2018-04-01 22:49:14.226083\n",
      "5000 Statuses Processed: 2018-04-01 22:49:15.137524\n",
      "5100 Statuses Processed: 2018-04-01 22:49:16.174315\n",
      "5200 Statuses Processed: 2018-04-01 22:49:16.896931\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'next'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-d51198a52b70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m     \u001b[0mscrapeFacebookPageFeedStatus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccess_token\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-d51198a52b70>\u001b[0m in \u001b[0;36mscrapeFacebookPageFeedStatus\u001b[1;34m(page_id, access_token)\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m'paging'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstatuses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m                 statuses = json.loads(request_until_succeed(\n\u001b[1;32m--> 146\u001b[1;33m                                         statuses['paging']['next']))\n\u001b[0m\u001b[0;32m    147\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m                 \u001b[0mhas_next_page\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'next'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "import csv\n",
    "import time\n",
    "\n",
    "#app_id = \"<FILL IN>\"\n",
    "#app_secret = \"<FILL IN>\" # DO NOT SHARE WITH ANYONE!\n",
    "page_id = input(\"Please Paste Public Page Name:\")\n",
    "\n",
    "#access_token = app_id + \"|\" + app_secret\n",
    "\n",
    "access_token = input(\"Please Paste Your Access Token:\")\n",
    "\n",
    "def request_until_succeed(url):\n",
    "    response = requests.get(url)\n",
    "    return response.text\n",
    "\n",
    "# Needed to write tricky unicode correctly to csv\n",
    "def unicode_normalize(text):\n",
    "    return text.translate({ 0x2018:0x27, 0x2019:0x27, 0x201C:0x22, 0x201D:0x22,\n",
    "                            0xa0:0x20 }).encode('utf-8')\n",
    "\n",
    "def getFacebookPageFeedData(page_id, access_token, num_statuses):\n",
    "\n",
    "    # Construct the URL string; see http://stackoverflow.com/a/37239851 for\n",
    "    # Reactions parameters\n",
    "    base = \"https://graph.facebook.com/v2.12\"\n",
    "    node = \"/%s/posts\" % page_id \n",
    "    fields = \"/?fields=message,link,permalink_url,created_time,type,name,id,\" + \\\n",
    "            \"comments.limit(0).summary(true),shares,reactions\" + \\\n",
    "            \".limit(0).summary(true)\"\n",
    "    parameters = \"&limit=%s&access_token=%s\" % (num_statuses, access_token)\n",
    "    url = base + node + fields + parameters\n",
    "\n",
    "    # retrieve data\n",
    "    data = json.loads(request_until_succeed(url))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def getReactionsForStatus(status_id, access_token):\n",
    "\n",
    "    # See http://stackoverflow.com/a/37239851 for Reactions parameters\n",
    "        # Reactions are only accessable at a single-post endpoint\n",
    "\n",
    "    base = \"https://graph.facebook.com/v2.6\"\n",
    "    node = \"/%s\" % status_id\n",
    "    reactions = \"/?fields=\" \\\n",
    "            \"reactions.type(LIKE).limit(0).summary(total_count).as(like)\" \\\n",
    "            \",reactions.type(LOVE).limit(0).summary(total_count).as(love)\" \\\n",
    "            \",reactions.type(WOW).limit(0).summary(total_count).as(wow)\" \\\n",
    "            \",reactions.type(HAHA).limit(0).summary(total_count).as(haha)\" \\\n",
    "            \",reactions.type(SAD).limit(0).summary(total_count).as(sad)\" \\\n",
    "            \",reactions.type(ANGRY).limit(0).summary(total_count).as(angry)\"\n",
    "    parameters = \"&access_token=%s\" % access_token\n",
    "    url = base + node + reactions + parameters\n",
    "\n",
    "    # retrieve data\n",
    "    data = json.loads(request_until_succeed(url))\n",
    "     \n",
    "    return data\n",
    "\n",
    "\n",
    "def processFacebookPageFeedStatus(status, access_token):\n",
    "\n",
    "    # The status is now a Python dictionary, so for top-level items,\n",
    "    # we can simply call the key.\n",
    "\n",
    "    # Additionally, some items may not always exist,\n",
    "    # so must check for existence first\n",
    "\n",
    "    status_id = status['id']\n",
    "    status_message = '' if 'message' not in status.keys() else \\\n",
    "            unicode_normalize(status['message'])\n",
    "    link_name = '' if 'name' not in status.keys() else \\\n",
    "            unicode_normalize(status['name'])\n",
    "    status_type = status['type']\n",
    "    status_link = '' if 'link' not in status.keys() else \\\n",
    "            unicode_normalize(status['link'])\n",
    "    status_permalink_url = '' if 'permalink_url' not in status.keys() else \\\n",
    "            unicode_normalize(status['permalink_url'])\n",
    "    # Time needs special care since a) it's in UTC and\n",
    "    # b) it's not easy to use in statistical programs.\n",
    "\n",
    "    status_published = datetime.datetime.strptime(\n",
    "            status['created_time'],'%Y-%m-%dT%H:%M:%S+0000')\n",
    "    status_published = status_published + \\\n",
    "            datetime.timedelta(hours=-5) # EST\n",
    "    status_published = status_published.strftime(\n",
    "            '%Y-%m-%d %H:%M:%S') # best time format for spreadsheet programs\n",
    "\n",
    "    # Nested items require chaining dictionary keys.\n",
    "\n",
    "    num_reactions = 0 if 'reactions' not in status else \\\n",
    "            status['reactions']['summary']['total_count']\n",
    "    num_comments = 0 if 'comments' not in status else \\\n",
    "            status['comments']['summary']['total_count']\n",
    "    num_shares = 0 if 'shares' not in status else status['shares']['count']\n",
    "\n",
    "    # Counts of each reaction separately; good for sentiment\n",
    "    # Only check for reactions if past date of implementation:\n",
    "    # http://newsroom.fb.com/news/2016/02/reactions-now-available-globally/\n",
    "\n",
    "    reactions = getReactionsForStatus(status_id, access_token) if \\\n",
    "            status_published > '2016-02-24 00:00:00' else {}\n",
    "\n",
    "    num_likes = 0 if 'like' not in reactions else \\\n",
    "            reactions['like']['summary']['total_count']\n",
    "\n",
    "    # Special case: Set number of Likes to Number of reactions for pre-reaction\n",
    "    # statuses\n",
    "\n",
    "    num_likes = num_reactions if status_published < '2016-02-24 00:00:00' \\\n",
    "            else num_likes\n",
    "\n",
    "    def get_num_total_reactions(reaction_type, reactions):\n",
    "        if reaction_type not in reactions:\n",
    "            return 0\n",
    "        else:\n",
    "            return reactions[reaction_type]['summary']['total_count']\n",
    "\n",
    "    num_loves = get_num_total_reactions('love', reactions)\n",
    "    num_wows = get_num_total_reactions('wow', reactions)\n",
    "    num_hahas = get_num_total_reactions('haha', reactions)\n",
    "    num_sads = get_num_total_reactions('sad', reactions)\n",
    "    num_angrys = get_num_total_reactions('angry', reactions)\n",
    "\n",
    "    # Return a tuple of all processed data\n",
    "\n",
    "    return (status_id, status_message, link_name, status_type, status_link, status_permalink_url,\n",
    "            status_published, num_reactions, num_comments, num_shares,\n",
    "            num_likes, num_loves, num_wows, num_hahas, num_sads, num_angrys)\n",
    "\n",
    "def scrapeFacebookPageFeedStatus(page_id, access_token):\n",
    "    with open('%s_facebook_statuses.csv' % page_id, 'w') as file:\n",
    "        w = csv.writer(file)\n",
    "        w.writerow([\"status_id\"])\n",
    "\n",
    "        has_next_page = True\n",
    "        num_processed = 0   # keep a count on how many we've processed\n",
    "        scrape_starttime = datetime.datetime.now()\n",
    "\n",
    "        print (\"Scraping %s Facebook Page: %s\\n\" % (page_id, scrape_starttime))\n",
    "\n",
    "        statuses = getFacebookPageFeedData(page_id, access_token, 100)\n",
    "\n",
    "        while has_next_page:\n",
    "            for status in statuses['data']:\n",
    "\n",
    "                # Ensure it is a status with the expected metadata\n",
    "                if 'reactions' in status:\n",
    "                    w.writerow(processFacebookPageFeedStatus(status,\n",
    "                        access_token))\n",
    "\n",
    "                # output progress occasionally to make sure code is not\n",
    "                # stalling\n",
    "                num_processed += 1\n",
    "                if num_processed % 100 == 0:\n",
    "                    print (\"%s Statuses Processed: %s\" % \\\n",
    "                        (num_processed, datetime.datetime.now()))\n",
    "\n",
    "            # if there is no next page, we're done.\n",
    "            if 'paging' in statuses.keys():\n",
    "                statuses = json.loads(request_until_succeed(\n",
    "                                        statuses['paging']['next']))\n",
    "            else:\n",
    "                has_next_page = False\n",
    "\n",
    "\n",
    "        print (\"\\nDone!\\n%s Statuses Processed in %s\" % \\\n",
    "                (num_processed, datetime.datetime.now() - scrape_starttime))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    scrapeFacebookPageFeedStatus(page_id, access_token)\n",
    "\n",
    "\n",
    "# The CSV can be opened in all major statistical programs. Have fun! :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Paste the Page name or Group ID:nsdbn\n",
      "Please Paste Your Access Token:ndn\n",
      "Scraping nsdbn Comments From Posts: 2018-04-02 01:02:01.205785\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'nsdbn_facebook_statuses.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f07dd341f989>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m     \u001b[0mscrapeFacebookPageFeedComments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccess_token\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-f07dd341f989>\u001b[0m in \u001b[0;36mscrapeFacebookPageFeedComments\u001b[1;34m(page_id, access_token)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Scraping %s Comments From Posts: %s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m                 \u001b[1;33m(\u001b[0m\u001b[0mfile_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscrape_starttime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%s_facebook_statuses.csv'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfile_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m             \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'nsdbn_facebook_statuses.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import datetime\n",
    "import csv\n",
    "import time\n",
    "\n",
    "#app_id = \"<FILL IN>\"\n",
    "#app_secret = \"<FILL IN>\" # DO NOT SHARE WITH ANYONE!\n",
    "file_id = input(\"Please Paste the Page name or Group ID:\")\n",
    "\n",
    "#access_token = app_id + \"|\" + app_secret\n",
    "access_token = input(\"Please Paste Your Access Token:\")\n",
    "\n",
    "def request_until_succeed(url):\n",
    "    req = requests.get(url)\n",
    "    return req.text\n",
    "\n",
    "# Needed to write tricky unicode correctly to csv\n",
    "def unicode_normalize(text):\n",
    "    return text.translate({ 0x2018:0x27, 0x2019:0x27, 0x201C:0x22, \n",
    "                            0x201D:0x22, 0xa0:0x20 }).encode('utf-8')\n",
    "\n",
    "def getFacebookCommentFeedData(status_id, access_token, num_comments):\n",
    "\n",
    "    # Construct the URL string\n",
    "        base = \"https://graph.facebook.com/v2.6\"\n",
    "        node = \"/%s/comments\" % status_id \n",
    "        fields = \"?fields=id,message,like_count,created_time,comments,from,attachment\"\n",
    "        parameters = \"&order=chronological&limit=%s&access_token=%s\" % \\\n",
    "                (num_comments, access_token)\n",
    "        url = base + node + fields + parameters\n",
    "\n",
    "        # retrieve data\n",
    "        data = request_until_succeed(url)\n",
    "        if data is None:\n",
    "            return None\n",
    "        else:   \n",
    "            return json.loads(data)\n",
    "\n",
    "def processFacebookComment(comment, status_id, parent_id = ''):\n",
    "\n",
    "    # The status is now a Python dictionary, so for top-level items,\n",
    "    # we can simply call the key.\n",
    "\n",
    "    # Additionally, some items may not always exist,\n",
    "    # so must check for existence first\n",
    "    print(comment)\n",
    "    comment_id = comment['id']\n",
    "    comment_message = '' if 'message' not in comment else \\\n",
    "            unicode_normalize(comment['message'])\n",
    "    comment_likes = 0 if 'like_count' not in comment else \\\n",
    "            comment['like_count']\n",
    "    comment_author = 0\n",
    "    if 'attachment' in comment:\n",
    "        attach_tag = \"[[%s]]\" % comment['attachment']['type'].upper()\n",
    "        comment_message = attach_tag if comment_message is '' else \\\n",
    "                (comment_message.decode(\"utf-8\") + \" \" + \\\n",
    "                        attach_tag).encode(\"utf-8\")\n",
    "\n",
    "    # Time needs special care since a) it's in UTC and\n",
    "    # b) it's not easy to use in statistical programs.\n",
    "\n",
    "    comment_published = datetime.datetime.strptime(\n",
    "            comment['created_time'],'%Y-%m-%dT%H:%M:%S+0000')\n",
    "    comment_published = comment_published + datetime.timedelta(hours=-5) # EST\n",
    "    comment_published = comment_published.strftime(\n",
    "            '%Y-%m-%d %H:%M:%S') # best time format for spreadsheet programs\n",
    "\n",
    "    # Return a tuple of all processed data\n",
    "\n",
    "    return (comment_id, status_id, parent_id, comment_message, comment_author,\n",
    "            comment_published, comment_likes)\n",
    "\n",
    "def scrapeFacebookPageFeedComments(page_id, access_token):\n",
    "    with open('%s_facebook_comments.csv' % file_id, 'w') as file:\n",
    "        w = csv.writer(file)\n",
    "        w.writerow([\"comment_id\", \"status_id\", \"parent_id\", \"comment_message\", \n",
    "            \"comment_author\", \"comment_published\", \"comment_likes\"])\n",
    "\n",
    "        num_processed = 0   # keep a count on how many we've processed\n",
    "        scrape_starttime = datetime.datetime.now()\n",
    "\n",
    "        print (\"Scraping %s Comments From Posts: %s\\n\" % \\\n",
    "                (file_id, scrape_starttime))\n",
    "\n",
    "        with open('%s_facebook_statuses.csv' % file_id, 'r') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "\n",
    "            #reader = [dict(status_id='759985267390294_1158001970921953')]\n",
    "\n",
    "            for status in reader:\n",
    "                has_next_page = True\n",
    "\n",
    "                comments = getFacebookCommentFeedData(status['status_id'], \n",
    "                        access_token, 100)\n",
    "                while has_next_page and comments is not None:\t\t\t\t\n",
    "                    for comment in comments['data']:\n",
    "                        w.writerow(processFacebookComment(comment, \n",
    "                            status['status_id']))\n",
    "\n",
    "                        if 'comments' in comment:\n",
    "                            has_next_subpage = True\n",
    "\n",
    "                            subcomments = getFacebookCommentFeedData(\n",
    "                                    comment['id'], access_token, 100)\n",
    "\n",
    "                            while has_next_subpage:\n",
    "                                for subcomment in subcomments['data']:\n",
    "                                    # print (processFacebookComment(\n",
    "                                        # subcomment, status['status_id'], \n",
    "                                        # comment['id']))\n",
    "                                    w.writerow(processFacebookComment(\n",
    "                                            subcomment, \n",
    "                                            status['status_id'], \n",
    "                                            comment['id']))\n",
    "\n",
    "                                    num_processed += 1\n",
    "                                    if num_processed % 1000 == 0:\n",
    "                                        print( \"%s Comments Processed: %s\" % \\\n",
    "                                                (num_processed, \n",
    "                                                    datetime.datetime.now()))\n",
    "\n",
    "                                if 'paging' in subcomments:\n",
    "                                    if 'next' in subcomments['paging']:\n",
    "                                        subcomments = json.loads(\n",
    "                                                request_until_succeed(\n",
    "                                                    subcomments['paging']\\\n",
    "                                                               ['next']))\n",
    "                                    else:\n",
    "                                        has_next_subpage = False\n",
    "                                else:\n",
    "                                    has_next_subpage = False\n",
    "\n",
    "                        # output progress occasionally to make sure code is not\n",
    "                        # stalling\n",
    "                        num_processed += 1\n",
    "                        if num_processed % 1000 == 0:\n",
    "                            print (\"%s Comments Processed: %s\" % \\\n",
    "                                    (num_processed, datetime.datetime.now()))\n",
    "\n",
    "                    if 'paging' in comments:\t\t\n",
    "                        if 'next' in comments['paging']:\n",
    "                            comments = json.loads(request_until_succeed(\n",
    "                                        comments['paging']['next']))\n",
    "                        else:\n",
    "                            has_next_page = False\n",
    "                    else:\n",
    "                        has_next_page = False\n",
    "\n",
    "\n",
    "        print (\"\\nDone!\\n%s Comments Processed in %s\" % \\\n",
    "                (num_processed, datetime.datetime.now() - scrape_starttime))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    scrapeFacebookPageFeedComments(file_id, access_token)\n",
    "\n",
    "\n",
    "# The CSV can be opened in all major statistical programs. Have fun! :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
