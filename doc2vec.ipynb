{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedLineDocument\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing = True # Set to True to use small files for quick testing, false to use the entire training data\n",
    "if (testing):\n",
    "    filename = 'Data/mergedDataSet_test.csv' #Sample file used for quick testing\n",
    "    commentsFileName = 'Data/comments_small.csv'\n",
    "else:\n",
    "    filename = 'Data/mergedDataSet.csv' #The complete file\n",
    "    commentsFileName = 'Data/comments.csv'\n",
    "\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nltk.download('stopwords') # Download it if not already downloaded\n",
    "stop = stopwords.words('english')\n",
    "def RemoveStopWords(row):\n",
    "    row = row.lower().split() # converting to lower case and splitting\n",
    "    str1 = ''\n",
    "    for item in row:\n",
    "        if item not in stop: #removing stop words\n",
    "            item = re.sub(r'[^\\w\\s]','',item) #removing punctutions\n",
    "            str1 += (item + ' ')\n",
    "    return str1\n",
    "target = df['merged_rating'].values #Saving the target variable as a numpy array\n",
    "df = df['comment_text'].apply(RemoveStopWords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(commentsFileName,index=False) # Writing the comments to a CSV file to be read by TaggedLineDocument next\n",
    "documents = TaggedLineDocument(commentsFileName) # Tags each sentence (0,1,2,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelSize = 500 # Will represent each comment with a vector of size 500\n",
    "modelWindow = 8 # Will look at 8 words together to do its calculations\n",
    "model = Doc2Vec(documents, modelSize, modelWindow, min_count=5, workers=4)\n",
    "# model.save('model_s500_w8') # Uncomment this to save the model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating a numpy array to keep the data and to be used by a machine learning model as the feature vector\n",
    "data = np.zeros(model.docvecs[0].shape)\n",
    "for i in range(len(model.docvecs)):\n",
    "    data = np.vstack((data,model.docvecs[i]))\n",
    "data = np.delete(data,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(data,target, test_size=0.2, random_state=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
